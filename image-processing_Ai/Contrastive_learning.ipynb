{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1WOlIqwzbmknNqGckR3Jtsa7vqmprFdpe","authorship_tag":"ABX9TyPIhCA6MAjteYlCrbmDXigm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#SimCLR_MoCo_SSL"],"metadata":{"id":"3hKmAF3Z4ag-"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","from torch.utils.data import DataLoader, random_split, Subset\n","from torchsummary import summary\n","import numpy as np\n","import random"],"metadata":{"id":"CrhCj2AXJ1zK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device: {device}')"],"metadata":{"id":"qnJ1j97vJ4Xf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734527757789,"user_tz":-210,"elapsed":573,"user":{"displayName":"Arian ghaderi","userId":"07749721859380516600"}},"outputId":"c0d91b0c-e44f-468a-c2da-bd7c8ddd8833"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"code","source":["# Hyperparameters\n","batch_size = 256  # Batch size for contrastive learning\n","batch_size_fine_tune = 32  # Batch size for fine-tuning\n","learning_rate = 0.001  # Learning rate\n","num_epochs = 100  # Number of epochs for contrastive learning\n","num_epochs_fine_tune = 10  # Number of epochs for fine-tuning\n","temperature = 0.5  # Temperature for NT-Xent loss"],"metadata":{"id":"XGUjIqF4J5hM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Transformations   (داده افزایی)      (agmention)\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),                    #رندوم کراپ زدن با سایز 224\n","    transforms.RandomHorizontalFlip(),                            #اینه کردن به روش هریزون تا\n","    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),                  #تغییر رنگ\n","    transforms.RandomGrayscale(p=0.2),                              #با احتمال 20 درصد تصویر رو سیاه سفید میکنه\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"4BgWdEhVJ6-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["این 4 تا اگمنتیشن ما میتونیم بیشتر هم اگمنتیشن یا همون داده افزایی رو داشته باشیم"],"metadata":{"id":"y6nAq-Vb7KTe"}},{"cell_type":"code","source":["# Load CIFAR-10 Dataset\n","dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n","train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"pCv7cKrWJ8N5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734528056726,"user_tz":-210,"elapsed":5823,"user":{"displayName":"Arian ghaderi","userId":"07749721859380516600"}},"outputId":"2342c6e1-2ad2-412c-b1b8-a2ded7e5dc53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:01<00:00, 98.8MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n"]}]},{"cell_type":"code","source":["# Model definition for SimCLR\n","class SimCLR(nn.Module):\n","    def __init__(self, base_model, out_dim):\n","        super(SimCLR, self).__init__()\n","        self.base_model = base_model        #مدل پایه که میتونه مثلا رزنت یا هرچی باشه\n","        self.features = nn.Sequential(*list(base_model.children())[:-1])    #ویژگی هامون\n","        self.projection_head = nn.Sequential(                           #تابع g ما که دوتا لایه خطی\n","            nn.Linear(base_model.fc.in_features, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, out_dim)\n","        )\n","\n","    def forward(self, x):\n","        h = self.features(x).squeeze()\n","        z = self.projection_head(h)\n","        return h, z"],"metadata":{"id":"9Dlen5tUJ90f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NT-Xent loss function                             #تابع ضرر و شباهت کسینوسی\n","def nt_xent_loss(z_i, z_j, temperature):\n","    N = z_i.shape[0]\n","    z = torch.cat((z_i, z_j), dim=0)\n","    sim = torch.mm(z, z.t()) / temperature\n","    sim_i_j = torch.diag(sim, N)\n","    sim_j_i = torch.diag(sim, -N)\n","    positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0)\n","    labels = torch.arange(N, device=z.device).repeat(2)\n","    loss = nn.CrossEntropyLoss()(sim, labels)\n","    return loss"],"metadata":{"id":"_f8u89LbJ_Xe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Training function for SimCLR\n","def train_simclr(model, train_loader, optimizer, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        for i, (inputs, _) in enumerate(train_loader):         #اینجا هم مثل یادگیری خود نظارتی برچسب نیاز نداریم\n","            optimizer.zero_grad()\n","            inputs = torch.cat([inputs, inputs], dim=0).to(device)      #concat input   256,3,224,224    256,3,244,224\n","            h, z = model(inputs)  #bordar h,z                                           #512,3,224,224\n","                                                                                   #x256==x512     x255==x511 ,...\n","            z_i, z_j = torch.split(z, batch_size, dim=0)         #میاد بردار زد که پونصد و دوازدتایی رو میشکونه به دوتا برداره(خط پایین)\n","            loss = nt_xent_loss(z_i, z_j, temperature)                    #  z_i=256,d,  z_j=256,d\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","            if (i + 1) % 10 == 0:\n","                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}')\n","                running_loss = 0.0"],"metadata":{"id":"0-apdu7AKC8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Tt-9G5SnWcO"},"outputs":[],"source":["# MoCo Model definition\n","class MoCo(nn.Module):\n","    def __init__(self, base_encoder, out_dim, K=4096, m=0.99, T=0.07):\n","        super(MoCo, self).__init__()\n","        self.K = K\n","        self.m = m\n","        self.T = T\n","\n","        # Create the encoders\n","        self.encoder_q = base_encoder(num_classes=out_dim)\n","        self.encoder_k = base_encoder(num_classes=out_dim)\n","\n","        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n","            param_k.data.copy_(param_q.data)  # initialize\n","            param_k.requires_grad = False  # not update by gradient\n","\n","        self.queue = nn.functional.normalize(torch.randn(out_dim, K), dim=0)\n","        self.queue = nn.functional.normalize(self.queue, dim=0)\n","        self.queue_ptr = 0\n","\n","    @torch.no_grad()\n","    def momentum_update_key_encoder(self):\n","        # update the key encoder\n","        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n","            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n","\n","    @torch.no_grad()\n","    def dequeue_and_enqueue(self, keys):\n","        batch_size = keys.shape[0]\n","        ptr = int(self.queue_ptr)\n","        assert self.K % batch_size == 0  # for simplicity\n","        self.queue[:, ptr:ptr + batch_size] = keys.T\n","        ptr = (ptr + batch_size) % self.K\n","        self.queue_ptr = ptr\n","\n","    def forward(self, im_q, im_k):\n","        q = self.encoder_q(im_q)  # queries: NxC\n","        q = nn.functional.normalize(q, dim=1)\n","\n","        with torch.no_grad():  # no gradient to keys\n","            self.momentum_update_key_encoder()  # update the key encoder\n","            k = self.encoder_k(im_k)  # keys: NxC\n","            k = nn.functional.normalize(k, dim=1)\n","\n","        # Compute logits\n","        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n","        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])\n","\n","        logits = torch.cat([l_pos, l_neg], dim=1)\n","        logits /= self.T\n","\n","        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(q.device)\n","        self.dequeue_and_enqueue(k)\n","\n","        return logits, labels\n","\n","\n","# Training function for MoCo\n","def train_moco(model, train_loader, optimizer, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        for i, (inputs, _) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            inputs = torch.cat([inputs, inputs], dim=0).to(device)\n","            im_q, im_k = torch.split(inputs, batch_size, dim=0)\n","            logits, labels = model(im_q, im_k)\n","            loss = nn.CrossEntropyLoss()(logits, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","            if (i + 1) % 10 == 0:\n","                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}')\n","                running_loss = 0.0"]}]}